---
title: æœ¬åœ°çŸ¥è¯†åº“æ­å»ºæŒ‡å—
description: å­¦ä¹ å¦‚ä½•æ„å»ºä¼ä¸šçº§æœ¬åœ°çŸ¥è¯†åº“ï¼Œå®ç°ç§æœ‰æ•°æ®çš„æ™ºèƒ½æ£€ç´¢ä¸é—®ç­”
category: è¿›é˜¶æ•™ç¨‹
tags: [çŸ¥è¯†åº“, RAG, å‘é‡æ•°æ®åº“, æ•°æ®å®‰å…¨]
updatedAt: 2025-02-09
---

# æœ¬åœ°çŸ¥è¯†åº“æ­å»ºæŒ‡å—

## æ¦‚è¿°

æœ¬åœ°çŸ¥è¯†åº“ï¼ˆLocal Knowledge Baseï¼‰æ˜¯ä¼ä¸š AI åº”ç”¨çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚é€šè¿‡å°†ä¼ä¸šç§æœ‰æ–‡æ¡£ã€æ•°æ®è½¬åŒ–ä¸ºå¯æ£€ç´¢çš„çŸ¥è¯†ï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹çš„ç†è§£èƒ½åŠ›ï¼Œå®ç°æ™ºèƒ½é—®ç­”ã€æ–‡æ¡£åˆ†æã€çŸ¥è¯†æŒ–æ˜ç­‰åŠŸèƒ½ã€‚

## æ ¸å¿ƒæ¶æ„

```mermaid
flowchart LR
    subgraph Input["æ•°æ®è¾“å…¥"]
        A[ğŸ“„ æ–‡æ¡£è¾“å…¥<br/>PDF/Word/ç­‰]
        F[ğŸ’¬ ç”¨æˆ·æé—®<br/>è‡ªç„¶è¯­è¨€]
    end
    
    subgraph Processing["å¤„ç†å±‚"]
        B[âš™ï¸ æ–‡æ¡£å¤„ç†å¼•æ“<br/>è§£æ/åˆ†å—/åµŒå…¥]
        D[ğŸ” RAG æ£€ç´¢å¼•æ“<br/>å‘é‡ç›¸ä¼¼åº¦æœç´¢]
    end
    
    subgraph Storage["å­˜å‚¨å±‚"]
        C[ğŸ—„ï¸ å‘é‡æ•°æ®åº“<br/>Milvus/PGVector]
    end
    
    subgraph Output["è¾“å‡ºå±‚"]
        E[ğŸ¤– å¤§è¯­è¨€æ¨¡å‹<br/>ç”Ÿæˆå›ç­”]
    end
    
    A --> B
    B --> C
    C --> D
    F --> D
    D --> E
```

## ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚

- **CPU**: 4æ ¸åŠä»¥ä¸Š
- **å†…å­˜**: 16GB åŠä»¥ä¸Š
- **å­˜å‚¨**: 100GB SSD åŠä»¥ä¸Š
- **GPU**: å¯é€‰ï¼Œæ¨èç”¨äº Embedding åŠ é€Ÿ

### ä¾èµ–å®‰è£…

```bash
# Python ç¯å¢ƒ
python -m pip install langchain langchain-community

# å‘é‡æ•°æ®åº“
pip install pymilvus  # æˆ– pgvector

# æ–‡æ¡£è§£æ
pip install pypdf unstructured

# Embedding æ¨¡å‹
pip install sentence-transformers
```

## å¿«é€Ÿå¼€å§‹

### 1. åˆå§‹åŒ–å‘é‡æ•°æ®åº“

ä½¿ç”¨ Milvus ä½œä¸ºå‘é‡å­˜å‚¨ï¼š

```bash
# Docker å¯åŠ¨ Milvus
docker-compose -f milvus-standalone-docker-compose.yml up -d
```

æˆ–ä½¿ç”¨ PostgreSQL + pgvectorï¼š

```sql
-- åˆ›å»ºå‘é‡æ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;

-- åˆ›å»ºçŸ¥è¯†åº“è¡¨
CREATE TABLE knowledge_base (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding VECTOR(768),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 2. æ–‡æ¡£å¤„ç†æµç¨‹

```python
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Milvus

# åŠ è½½æ–‡æ¡£
loader = DirectoryLoader(
    "./documents",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader
)
documents = loader.load()

# æ–‡æ¡£åˆ†å—
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", "ã€‚", "ï¼›", " "]
)
chunks = text_splitter.split_documents(documents)

# åˆ›å»º Embedding
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5"
)

# å­˜å…¥å‘é‡åº“
vector_store = Milvus.from_documents(
    documents=chunks,
    embedding=embeddings,
    connection_args={"host": "localhost", "port": "19530"},
    collection_name="company_knowledge"
)
```

### 3. æ£€ç´¢ä¸é—®ç­”

```python
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# åˆ›å»ºæ£€ç´¢å™¨
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# æ„å»º RAG é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# æé—®
result = qa_chain({"query": "å…¬å¸çš„å¹´å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"})
print(result["result"])
```

## é«˜çº§é…ç½®

### æ··åˆæ£€ç´¢ç­–ç•¥

ç»“åˆå‘é‡æ£€ç´¢ä¸å…³é”®è¯æ£€ç´¢ï¼Œæå‡å‡†ç¡®æ€§ï¼š

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# BM25 å…³é”®è¯æ£€ç´¢
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 5

# å‘é‡æ£€ç´¢
vector_retriever = vector_store.as_retriever(search_kwargs={"k": 5})

# èåˆæ£€ç´¢
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.3, 0.7]
)
```

### é‡æ’åºä¼˜åŒ–

ä½¿ç”¨é‡æ’åºæ¨¡å‹æå‡æ£€ç´¢è´¨é‡ï¼š

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker

# é‡æ’åºæ¨¡å‹
reranker = CrossEncoderReranker(model="BAAI/bge-reranker-large")

# å‹ç¼©æ£€ç´¢å™¨
compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=vector_retriever
)
```

### å¤šç§Ÿæˆ·éš”ç¦»

ä¼ä¸šåœºæ™¯ä¸‹çš„æ•°æ®éš”ç¦»æ–¹æ¡ˆï¼š

```python
# ä¸ºæ¯ä¸ªç§Ÿæˆ·åˆ›å»ºç‹¬ç«‹ Collection
from pymilvus import Collection, FieldSchema, CollectionSchema, DataType

def create_tenant_collection(tenant_id):
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
        FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
        FieldSchema(name="metadata", dtype=DataType.JSON),
        FieldSchema(name="tenant_id", dtype=DataType.VARCHAR, max_length=64)
    ]
    
    schema = CollectionSchema(fields)
    collection = Collection(f"knowledge_{tenant_id}", schema)
    
    # åˆ›å»ºç´¢å¼•
    index_params = {
        "metric_type": "COSINE",
        "index_type": "IVF_FLAT",
        "params": {"nlist": 1024}
    }
    collection.create_index("embedding", index_params)
    return collection
```

## æ€§èƒ½ä¼˜åŒ–

### 1. Embedding æ¨¡å‹é€‰æ‹©

| æ¨¡å‹ | ç»´åº¦ | è¯­è¨€ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| BAAI/bge-large-zh | 1024 | ä¸­æ–‡ | é€šç”¨ä¸­æ–‡åœºæ™¯ |
| BAAI/bge-m3 | 1024 | å¤šè¯­è¨€ | å¤šè¯­è¨€æ··åˆ |
| text-embedding-3 | 1536 | å¤šè¯­è¨€ | OpenAI ç”Ÿæ€ |
| m3e-base | 768 | ä¸­æ–‡ | è½»é‡çº§åº”ç”¨ |

### 2. åˆ†å—ç­–ç•¥ä¼˜åŒ–

```python
# æŒ‰è¯­ä¹‰åˆ†å—
from langchain.text_splitter import SemanticChunker

semantic_splitter = SemanticChunker(
    embeddings,
    breakpoint_threshold_type="percentile",
    breakpoint_threshold_amount=95
)

# æŒ‰ç»“æ„åˆ†å—ï¼ˆMarkdownï¼‰
from langchain.text_splitter import MarkdownHeaderTextSplitter

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[("#", "Header 1"), ("##", "Header 2")]
)
```

### 3. ç¼“å­˜ç­–ç•¥

```python
from langchain.cache import InMemoryCache
import langchain

# å¯ç”¨ç¼“å­˜
langchain.llm_cache = InMemoryCache()

# æˆ–ä½¿ç”¨ Redis ç¼“å­˜
from langchain.cache import RedisCache
import redis

redis_client = redis.Redis(host='localhost', port=6379)
langchain.llm_cache = RedisCache(redis_client)
```

## å®‰å…¨ä¸éšç§

### æ•°æ®è„±æ•

```python
import re

def desensitize_text(text):
    # æ‰‹æœºå·è„±æ•
    text = re.sub(r'1[3-9]\d{9}', lambda m: m.group()[:3] + '****' + m.group()[7:], text)
    # èº«ä»½è¯å·è„±æ•
    text = re.sub(r'\d{17}[\dXx]', lambda m: m.group()[:6] + '********' + m.group()[14:], text)
    # é‚®ç®±è„±æ•
    text = re.sub(r'(\w{2})\w+@(\w+)', r'\1***@\2', text)
    return text

# å¤„ç†å‰å¯¹æ–‡æ¡£è¿›è¡Œè„±æ•
chunks = [desensitize_text(chunk) for chunk in chunks]
```

### è®¿é—®æ§åˆ¶

```python
from functools import wraps

def require_permission(permission):
    def decorator(func):
        @wraps(func)
        def wrapper(user, *args, **kwargs):
            if permission not in user.permissions:
                raise PermissionError("Access denied")
            return func(user, *args, **kwargs)
        return wrapper
    return decorator

@require_permission("knowledge:read")
def query_knowledge_base(user, query):
    # æŸ¥è¯¢é€»è¾‘
    pass
```

## æœ€ä½³å®è·µ

### æ–‡æ¡£é¢„å¤„ç†æ¸…å•

- [ ] æ‰«æä»¶éœ€å…ˆ OCR è¯†åˆ«
- [ ] å»é™¤é¡µçœ‰é¡µè„šå’Œé¡µç 
- [ ] è¡¨æ ¼å†…å®¹è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
- [ ] å›¾ç‰‡æå–å¹¶æ·»åŠ æè¿°
- [ ] æ•æ„Ÿä¿¡æ¯è„±æ•å¤„ç†

### æŒç»­ä¼˜åŒ–

1. **ç”¨æˆ·åé¦ˆæ”¶é›†**: è®°å½•é—®ç­”è´¨é‡ï¼Œæ ‡è®°é”™è¯¯å›ç­”
2. **å¢é‡æ›´æ–°**: å®šæœŸå¤„ç†æ–°å¢æ–‡æ¡£ï¼Œæ— éœ€å…¨é‡é‡å»º
3. **ç›‘æ§æŒ‡æ ‡**: æ£€ç´¢å»¶è¿Ÿã€å›ç­”å‡†ç¡®ç‡ã€ç”¨æˆ·æ»¡æ„åº¦
4. **A/B æµ‹è¯•**: å¯¹æ¯”ä¸åŒ Embedding æ¨¡å‹å’Œå‚æ•°æ•ˆæœ

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•å¤„ç†æ‰«æç‰ˆ PDFï¼Ÿ

ä½¿ç”¨ OCR å·¥å…·é¢„å¤„ç†ï¼š

```python
import pytesseract
from pdf2image import convert_from_path

images = convert_from_path("scan.pdf")
text = "\n".join([pytesseract.image_to_string(img, lang='chi_sim+eng') for img in images])
```

### Q: çŸ¥è¯†åº“æ›´æ–°å¦‚ä½•å¢é‡å¤„ç†ï¼Ÿ

```python
# è®¡ç®—æ–‡æ¡£æŒ‡çº¹
import hashlib

def get_doc_fingerprint(filepath):
    with open(filepath, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

# åªå¤„ç†æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡æ¡£
existing_ids = set(get_existing_ids())
new_docs = [doc for doc in all_docs if doc.id not in existing_ids]
```

### Q: å¦‚ä½•å¤„ç†è¶…é•¿æ–‡æ¡£ï¼Ÿ

é‡‡ç”¨åˆ†å±‚æ‘˜è¦ç­–ç•¥ï¼š
1. æ®µè½çº§æ‘˜è¦
2. ç« èŠ‚çº§æ‘˜è¦  
3. æ–‡æ¡£çº§æ‘˜è¦
4. æ„å»ºæ ‘çŠ¶ç´¢å¼•ç»“æ„

## æ€»ç»“

æœ¬åœ°çŸ¥è¯†åº“æ˜¯ä¼ä¸š AI åº”ç”¨çš„åŸºç¡€è®¾æ–½ï¼Œé€šè¿‡åˆç†çš„æ¶æ„è®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–å’Œå®‰å…¨ç­–ç•¥ï¼Œå¯ä»¥æ„å»ºé«˜æ•ˆã€å¯é çš„æ™ºèƒ½çŸ¥è¯†ç®¡ç†ç³»ç»Ÿã€‚

å¦‚éœ€æŠ€æœ¯æ”¯æŒï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ä»¬ï¼š
- é‚®ç®±ï¼šc@m9ai.work
- å®˜ç½‘ï¼šhttps://m9ai.work
